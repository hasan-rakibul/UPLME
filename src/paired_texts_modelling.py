import torch
from torch import Tensor
import torch.nn.functional as F
import lightning as L
from transformers import (
    AutoModel, 
    get_linear_schedule_with_warmup
)
from torchmetrics.functional import pearson_corrcoef, concordance_corrcoef, mean_squared_error, spearman_corrcoef
import logging
import numpy as np
import os
from pathlib import Path
import pandas as pd
from zipfile import ZipFile

from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor
from lightning.pytorch.loggers import WandbLogger

import optuna
from optuna.integration import PyTorchLightningPruningCallback
import plotly # required for optuna.visualisation
from functools import partial

from preprocess import PairedTextDataModule
from utils import log_info, plot_uncertainy, process_seedwise_metrics, DelayedStartEarlyStopping, beta_nll_loss
from util_uncertainty_metrics import calculate_unc_metrics

logger = logging.getLogger(__name__)
    
class CrossEncoderProbModel(torch.nn.Module):
    def __init__(self, plm_name: str):
        super().__init__()
        self.model = AutoModel.from_pretrained(plm_name, trust_remote_code=True)

        if plm_name.startswith("roberta"):
            # only applicable for roberta
            self.pooling = "roberta-pooler"
        else:
            self.pooling = "cls"

        self.out_proj_m = torch.nn.Sequential(
            torch.nn.LayerNorm(self.model.config.hidden_size),
            torch.nn.Dropout(0.25),
            torch.nn.Linear(self.model.config.hidden_size, 1)
        )

        self.out_proj_v = torch.nn.Sequential(
            torch.nn.LayerNorm(self.model.config.hidden_size),
            torch.nn.Dropout(0.25),
            torch.nn.Linear(self.model.config.hidden_size, 1),
            torch.nn.Softplus()
        )

    def forward(self, input_ids, attention_mask):
        output = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        if self.pooling == "mean":
            sentence_representation = (
                (output.last_hidden_state * attention_mask.unsqueeze(-1)).sum(-2) /
                attention_mask.sum(dim=-1).unsqueeze(-1)
            )
        elif self.pooling == "cls":
            sentence_representation = output.last_hidden_state[:, 0, :]
        elif self.pooling == "roberta-pooler":
            sentence_representation = output.pooler_output # (batch_size, hidden_dim)

        mean = self.out_proj_m(sentence_representation)
        var = self.out_proj_v(sentence_representation)
        var = torch.clamp(var, min=1e-8, max=1000) # following Seitzer-NeurIPS2022
        
        return mean.squeeze(), var.squeeze(), sentence_representation, output.last_hidden_state

class CrossEncoderBasicModel(torch.nn.Module):
    def __init__(self, plm_name: str):
        super().__init__()
        self.model = AutoModel.from_pretrained(plm_name)
        self.pooling = "roberta-pooler"

        self.out_proj = torch.nn.Sequential(
            torch.nn.LayerNorm(self.model.config.hidden_size),
            torch.nn.Dropout(0.25),
            torch.nn.Linear(self.model.config.hidden_size, 1)
        )

    def forward(self, batch):
        output = self.model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask']
        )

        if self.pooling == "mean":
            attn = batch['attention_mask']
            sentence_representation = (output.last_hidden_state * attn.unsqueeze(-1)).sum(-2) / attn.sum(dim=-1).unsqueeze(-1)
        elif self.pooling == "cls":
            sentence_representation = output.last_hidden_state[:, 0, :]
        elif self.pooling == "roberta-pooler":
            sentence_representation = output.pooler_output

        y_hat = self.out_proj(sentence_representation) 

        return (
            y_hat.squeeze(), 
            output.last_hidden_state
        )
    
class LitPairedTextModel(L.LightningModule):
    def __init__(
        self,
        plm_names: list[str],
        lr: float,
        log_dir: str,
        save_uc_metrics: bool,
        error_decay_factor: float,
        approach: str,
        sep_token_id: int, # required for alignment loss
        lambdas: list[float] = [], # initlisaed to compatible with old saved checkpoints
        num_passes: int = 4,
        wd: float = 0.1
    ):
        super().__init__()
        self.save_hyperparameters()

        self.plm_names = plm_names

        self.approach = approach
        if self.approach == "cross-basic":
            self.model = CrossEncoderBasicModel(plm_name=self.plm_names[0])
        elif self.approach == "cross-prob":
            self.model = CrossEncoderProbModel(plm_name=self.plm_names[0])
        else:
            raise ValueError(f"Invalid approach: {self.approach}")

        self.lr = lr
        self.wd = wd
        self.log_dir = log_dir
        self.save_uc_metrics = save_uc_metrics

        self.error_decay_factor = error_decay_factor
        
        self.lambdas = lambdas
        self.sep_token_id = sep_token_id
        self.num_passes = num_passes
        
        self.penalty_type = "exp-decay"

        self.validation_outputs = []
        self.test_outputs = []

    def configure_optimizers(self):
        optimiser = torch.optim.AdamW(
            params=self.parameters(),
            lr=self.lr,
            betas=(0.9, 0.98),
            eps=1e-6,
            weight_decay=self.wd
        )

        # Caution: self.trainer.num_training_batchs is inf if train_dataloader is not loaded
        # calculating num_training_steps using self.trainer.estimated_stepping_batches loads the train_dataloader
        # so, the below code "in the following order" is important
        num_training_steps = self.trainer.estimated_stepping_batches
        num_warmup_steps = int(0.03 * num_training_steps)
        # moved to 3% from 6% as I doubled the number of steps (epochs) compared to RoBERTa paper
        
        # RoBERTa paper used 10 epoch for fine-tuning, so I can calculate the number of warmup steps using 10 epochs
        # Also note num_training_batch is inf if I only set max_steps without max_epochs (I guess)
        # num_warmup_steps = int(0.06 * self.trainer.num_training_batches * 10)
        
        log_info(logger, f"Num training steps: {num_training_steps}")
        log_info(logger, f"Num warmup steps: {num_warmup_steps}")
        lr_scheduler = get_linear_schedule_with_warmup(
            optimiser,
            num_training_steps=num_training_steps,
            num_warmup_steps=num_warmup_steps
        )

        return {
            "optimizer": optimiser,
            "lr_scheduler": {
                "scheduler": lr_scheduler,
                "interval": "step",
                "frequency": 1 
            }
        }
    
    def _compute_penalty_loss(self, mean: Tensor, var: Tensor, labels: Tensor) -> Tensor:
        errors = (mean - labels) ** 2

        if self.penalty_type == "exp-decay":
            weight = torch.exp(-self.error_decay_factor * errors)
            var = var * weight
            penalty_loss = torch.linalg.norm(var, ord=2) / var.numel()
        else:
            raise ValueError(f"Invalid penalty type: {self.penalty_type}")

        return penalty_loss

    def _rescale_labels(self, labels: Tensor) -> Tensor:
        if "newsemp" in self.log_dir: # very loose check. TODO: take a variable input; not done now because then I will not be able to load saved models
            labels = (labels.float() - 4.0) / 3.0 # 1 is -1, 4 is 0, 7 is 1; like cos_sim
        elif "empstories" in self.log_dir:
            # generic, but not tested on newsemp
            min_label = 1.0
            max_label = 4.0
            labels = 2 * (labels.float() - min_label) / (max_label - min_label) - 1
        else:
            raise ValueError(f"Alignment loss requires newsemp or empstories in the log directory name. Got {self.log_dir}")
        return labels
    
    def _compute_alignment_betn_texts(self, input_ids: Tensor, hidden_state: Tensor, labels: Tensor) -> Tensor:
        # input_ids: (bsz, seq_len)
        # hidden_state: (bsz, seq_len, hidden_dim)
        bsz = input_ids.shape[0]

        input1_reprs, input2_reprs = [], []
        for i in range(bsz):
            # doing for each sample in the batch
            input_ids_sample = input_ids[i] # (seq_len)
            h_i = hidden_state[i] # (seq_len, hidden_dim)

            sep_positions = (input_ids_sample == self.sep_token_id).nonzero(as_tuple=True)[0]

            if "deberta" in self.plm_names[0] or "ModernBERT" in self.plm_names[0]:
                # token pattern: [CLS] input1 [SEP] input2 [SEP]
                assert len(sep_positions) == 2, f"Number of sep positions is not 2. {sep_positions}"
                first_sep = sep_positions[0]
                second_sep = sep_positions[1]

                input1_repr = h_i[1:first_sep] # excluding special tokens, (first_sep-1, hidden_dim)
                input2_repr = h_i[first_sep+1:second_sep] # excluding special tokens, (second_sep-first_sep-1, hidden_dim)
                # ^ important to not include till -1 because there could be padded stuffs

            elif "roberta" in self.plm_names[0]:
                # token pattern: [CLS] input1 [SEP][SEP] input2 [SEP]
                assert len(sep_positions) == 3, f"Number of sep positions is not 3. {sep_positions}"
                first_sep = sep_positions[0]
                second_sep = sep_positions[1]
                third_sep = sep_positions[2]
                
                input1_repr = h_i[1:first_sep] # excluding special tokens, (first_sep-1, hidden_dim)
                input2_repr = h_i[second_sep+1:third_sep] # excluding special tokens, (third_sep-second_sep-1, hidden_dim)
                
            else:
                raise NotImplementedError(f"Alignment loss is not implemented for {model_family} model family.")
            
            # Pool representation
            input1_repr = input1_repr.mean(dim=0) # (hidden_dim)
            input2_repr = input2_repr.mean(dim=0)

            input1_reprs.append(input1_repr)
            input2_reprs.append(input2_repr)
        
        input1_reprs = torch.stack(input1_reprs) # (bsz, hidden_dim)
        input2_reprs = torch.stack(input2_reprs)

        cos_sim = F.cosine_similarity(input1_reprs, input2_reprs)
                
        labels = self._rescale_labels(labels)

        loss = F.mse_loss(cos_sim, labels)
        return loss
    
    def _compute_loss(self, mean: Tensor, var: Tensor, labels: Tensor, prefix: str,
                      input_ids: Tensor, hidden_state: Tensor) -> tuple[Tensor, dict]:
        loss_dict = {}
        
        if self.approach == "cross-basic":
            # Basic model with MSE loss
            total_loss = F.mse_loss(mean, labels.squeeze())
            loss_dict[f"{prefix}_mse"] = total_loss.item()
        else:
            total_loss = 0 # initiliasing to be safe, required when both lambdas are 0
            if self.lambdas[0] != 0:
                nll_loss = self.lambdas[0] * beta_nll_loss(mean, var, labels.squeeze())
                loss_dict[f"{prefix}_nll"] = nll_loss.item()
            else:
                nll_loss = 0

            if self.lambdas[1] != 0:
                penalty_loss = self.lambdas[1] * self._compute_penalty_loss(mean=mean, var=var, labels=labels)
                loss_dict[f"{prefix}_penalty"] = penalty_loss.item()
            else:
                penalty_loss = 0

            total_loss = nll_loss + penalty_loss

        if self.lambdas[2] != 0:
            assert hidden_state is not None, "Hidden state is required for alignment loss"
            alignment_loss = self.lambdas[2] * self._compute_alignment_betn_texts(
                input_ids=input_ids,
                hidden_state=hidden_state,
                labels=labels
            )
            loss_dict[f"{prefix}_alignment"] = alignment_loss.item()

            total_loss += alignment_loss
            loss_dict[f"{prefix}_loss"] = total_loss.item()

        return total_loss, loss_dict
    
    def forward(self, batch: dict) -> tuple[Tensor, Tensor, Tensor]:
        means, varss, hidden_states = [], [], []

        for _ in range(self.num_passes):
            if self.approach == "cross-prob":
                mean, var, _, hidden_state = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask']
                )
            elif self.approach == "cross-basic":
                mean, hidden_state = self.model(batch)
                var = torch.zeros_like(mean)
            
            means.append(mean)
            varss.append(var)
            hidden_states.append(hidden_state)
        
        mean = torch.stack(means, dim=0).mean(dim=0)
        var = torch.stack(varss, dim=0).mean(dim=0)
        hidden_state = torch.stack(hidden_states, dim=0).mean(dim=0)

        return mean, var, hidden_state
    
    def training_step(self, batch, batch_idx):
        mean, var, hidden_state = self.forward(batch)
        
        loss, loss_dict = self._compute_loss(mean, var, batch["labels"], prefix="train",
                                             input_ids=batch['input_ids'],
                                             hidden_state=hidden_state)
        self.log_dict(
            loss_dict,
            on_step=True,
            on_epoch=False,
            logger=True,
            prog_bar=False,
            sync_dist=True,
            batch_size=batch["labels"].shape[0]
        )
        return loss
    
    def _enable_dropout_at_inference(self):
        for m in self.model.modules():
            if isinstance(m, torch.nn.Dropout):
                m.train()

    def validation_step(self, batch, batch_idx):
        if self.num_passes > 1:
            self._enable_dropout_at_inference()

        mean, var, hidden_state = self.forward(batch)

        # Note: it's important to check dim and unsqeeze as we get 0-dim if the last batch has only one sample
        # Otherwise, we get an error when concatenating tensors later
        mean = mean.unsqueeze(0) if mean.dim() == 0 else mean
        var = var.unsqueeze(0) if var.dim() == 0 else var
        labels = batch["labels"].unsqueeze(0) if batch["labels"].dim() == 0 else batch["labels"]
        self.validation_outputs.append({
            "mean": mean,
            "var": var,
            "labels": labels
        })

        _, loss_dict = self._compute_loss(
            mean=mean, var=var, labels=labels, prefix="val",
            input_ids=batch['input_ids'], hidden_state=hidden_state
        )
        self.log_dict(
            loss_dict,
            on_step=False,
            on_epoch=True,
            logger=True,
            prog_bar=False,
            sync_dist=True,
            batch_size=labels.shape[0]
        )


    def _calculate_metrics(self, mean: Tensor, var: Tensor, label: Tensor, mode: str) -> dict:
        # requires the tensors to be on the correct device as these are used for early stopping, checkpointing, etc.
        pcc = pearson_corrcoef(mean, label).to(self.device)
        ccc = concordance_corrcoef(mean, label).to(self.device)
        scc = spearman_corrcoef(mean, label).to(self.device)
        rmse = mean_squared_error(mean, label, squared=False).to(self.device)

        metrics_dict = {
            f"{mode}_pcc": pcc.item(),
            f"{mode}_ccc": ccc.item(),
            f"{mode}_scc": scc.item(),
            f"{mode}_rmse": rmse.item()
        }

        if self.approach != "cross-basic":
            # meaning that the model is probabilistic
            # In my understanding, it is fine to have unc_metrics in CPU as it's not used for any further computation
            unc_metrics = calculate_unc_metrics(mean=mean, var=var, label=label)
            unc_metrics = {f"{mode}_{k}": v for k, v in unc_metrics.items()}
            metrics_dict.update(unc_metrics)
        
        return metrics_dict
    
    def on_validation_epoch_end(self):
        all_means = torch.cat([out["mean"] for out in self.validation_outputs])
        all_labels = torch.cat([out["labels"] for out in self.validation_outputs])
        all_vars = torch.cat([out["var"] for out in self.validation_outputs])

        all_means = all_means.to(torch.float64).cpu()
        all_labels = all_labels.to(torch.float64).cpu()
        all_vars = all_vars.to(torch.float64).cpu()

        metric_dict = self._calculate_metrics(mean=all_means, var=all_vars, label=all_labels, mode="val")

        self.log_dict(
            metric_dict,
            on_step=False, # must be False
            on_epoch=True, # must be True
            logger=True,
            prog_bar=False,
            sync_dist=True,
            batch_size=self.validation_outputs[0]["mean"].shape[0]
        )

        self.validation_outputs.clear()

    def _save_and_plot_uncertainty(self, output: list[dict]):
        # convert tensors to numpy arrays
        output_dict = {}
        for out in output:
            for key in out:
                array = out[key].cpu().numpy()
                if key in output_dict:
                    output_dict[key] = np.concatenate((output_dict[key], array), axis=0)
                else:
                    output_dict[key] = array
        
        # save for later use
        save_as = f"{self.log_dir}/outputs.npy"
        if os.path.exists(save_as):
            save_as = f"{self.log_dir}/outputs_new.npy"
        np.save(save_as, output_dict)
        log_info(logger, f"Saved output to {save_as}")

        if "labels" in output_dict:
            plot_uncertainy(output_dict, f"{self.log_dir}/uncertainty.pdf")

    def test_step(self, batch, batch_idx):
        if self.num_passes > 1:
            self._enable_dropout_at_inference()

        mean, var, _ = self.forward(batch)

        outputs = {
            "mean": mean.unsqueeze(0) if mean.dim() == 0 else mean,
            "var": var.unsqueeze(0) if var.dim() == 0 else var
        }

        if "labels" in batch:
            outputs["labels"] = batch["labels"]
        
        if "noise" in batch:
            outputs["noise"] = batch["noise"]

        self.test_outputs.append(outputs)

    def on_test_epoch_end(self):
        all_means = torch.cat([out["mean"] for out in self.test_outputs])
        all_means = all_means.to(torch.float64).cpu()
        
        all_vars = torch.cat([out["var"] for out in self.test_outputs])
        all_vars = all_vars.to(torch.float64).cpu()

        if "labels" in self.test_outputs[0]:
            all_labels = torch.cat([out["labels"] for out in self.test_outputs])
            all_labels = all_labels.to(torch.float64).cpu()

            self.log_dict(
                self._calculate_metrics(mean=all_means, var=all_vars, label=all_labels, mode="test"),
                logger=False,
                prog_bar=False,
                sync_dist=True,
                batch_size=self.test_outputs[0]["mean"].shape[0]
            )
        else:
            log_info(logger, "No labels found in test outputs. Assuming metrics to be calculated separately. Saving predictions.")
            pred_df = pd.DataFrame({'emp': all_means, 'dis': all_means}) # we're not predicting distress, just aligning with submission system
            save_as = f"{self.log_dir}/predictions_EMP.tsv"
            pred_df.to_csv(save_as, sep='\t', index=None, header=None)
            # CodaLab requires zip file
            save_as = f"{self.log_dir}/predictions.zip"
            with ZipFile(save_as, 'w') as zipf:
                zipf.write(f"{self.log_dir}/predictions_EMP.tsv", arcname="predictions_EMP.tsv") # TODO: not tested after adding arcname, but it is required to avoid the file being saved in a subdirectory
            log_info(logger, f"Saved predictions to {save_as}")
            # TODO: if the above zip is successful, then remove the tsv file

        if self.approach != "cross-basic":
            # meaning that the model is probabilistic
            self._save_and_plot_uncertainty(self.test_outputs)

        self.test_outputs.clear()

class PairedTextModelController(object):
    def __init__(
        self,
        labelled_train_files: list[str],
        val_files: list[str],
        test_files: list[str],
        lr: float,
        train_bsz: int,
        eval_bsz: int,
        # num_epochs: int,
        max_steps: int,
        val_check_interval: int | float,
        noise_level: float,
        delta: float | None,
        expt_name: str,
        debug: bool,
        do_tune: bool = False,
        do_train: bool = True,
        do_test: bool = False,
        error_decay_factor: float = 0.5,
        lambdas: list[float] = [1.0],
        approach: str = "cross-prob",
        main_data: str = "newsemp",
        lbl_split: float = 1.0,
        plm_names: list[str] = ["roberta-base"],
        num_passes: int = 1,
        sanitise_labels: bool = False,
        add_noise_train: bool = False,
        add_noise_test: bool = False,
        do_augment: bool = False,
        wd: float = 0.1
    ):
        self.train_file = labelled_train_files
        self.val_file = val_files
        self.test_file = test_files
         
        self.lr = lr
        self.wd = wd
        self.train_bsz = train_bsz
        self.eval_bsz = eval_bsz
        self.max_steps = max_steps
        self.val_check_interval = val_check_interval

        self.noise_level = noise_level
        self.delta = delta
        self.expt_name = expt_name
        self.debug = debug
        self.do_tune = do_tune
        self.do_train = do_train
        self.do_test = do_test
        self.error_decay_factor = error_decay_factor
        self.lambdas = lambdas
        self.approach = approach
        self.num_passes = num_passes
        self.sanitise_labels = sanitise_labels
        self.add_noise_train = add_noise_train
        self.do_augment = do_augment

        self.enable_early_stopping = True
        self.early_stopping_start_epoch = 5 # applicable for DelayedStartEarlyStopping
        self.enable_checkpointing = True
        self.save_uc_metrics = False

        self.plm_names = plm_names
        self.lbl_split = lbl_split

        self.dm = PairedTextDataModule(
            noise_level=self.noise_level,
            delta=self.delta,
            tokeniser_plms=self.plm_names,
            tokenise_paired_texts_each_tokeniser=True \
                if self.approach in ["cross-basic", "cross-prob"] else False
            # TODO: check if the above should be True for other approaches
            # is_separate_tokeniser=True if self.approach in ["bi-prob", "siamese"] else False # NOTE: moving to len(plm_names) based decision
        )

        # train_dl is seed-dependent, so it's created in the seed-wise training
        self.is_newsemp_main = (main_data == "newsemp")
        if self.do_train or self.do_tune:
            self.val_dl = self.dm.get_val_dl(
                data_path_list=self.val_file,
                batch_size=self.eval_bsz,
                sanitise_newsemp_labels=self.sanitise_labels,
                add_noise=False,
                is_newsemp=self.is_newsemp_main
            )
        if self.do_test:
            self.test_dl = self.dm.get_test_dl(
                data_path_list=self.test_file, batch_size=self.eval_bsz, 
                sanitise_newsemp_labels=self.sanitise_labels, add_noise=add_noise_test,
                is_newsemp=self.is_newsemp_main
            )
           
    def _prepare_trainer(self, curr_log_dir: str, extra_callbacks: list | None = None):
        callbacks = []

        if self.enable_early_stopping:
            early_stopping = DelayedStartEarlyStopping(
                start_epoch=self.early_stopping_start_epoch,
                monitor="val_ccc",
                patience=2,
                mode="max",
                min_delta=0,
                verbose=True
            )

            callbacks.append(early_stopping)
        else:
            log_info(logger, "Early stopping disabled")

        if self.enable_checkpointing:            
            # last only
            checkpoint = ModelCheckpoint(
                save_top_k=1 # saves the last checkpoint; no need to save_last=True as it will save another checkpoint unnecessarily
            )
            
            callbacks.append(checkpoint)

        lr_monitor = LearningRateMonitor()
        callbacks.append(lr_monitor)

        callbacks.extend(extra_callbacks) if extra_callbacks else None

        wandb_logger = WandbLogger(
            name=self.expt_name,
            project="NoisEmpathy",
            save_dir=curr_log_dir,
            offline=self.debug
        )

        trainer = L.Trainer(
            # max_epochs=self.num_epochs,
            val_check_interval=self.val_check_interval,
            check_val_every_n_epoch=None, # eniirely steps-based validation
            max_steps=self.max_steps,
            default_root_dir=curr_log_dir,
            deterministic=True,
            logger=wandb_logger,
            log_every_n_steps=50, # frequent logging will require more memory; watch video at https://lightning.ai/docs/pytorch/stable/common/trainer.html#log-every-n-steps
            callbacks=callbacks,
            devices="auto",
            enable_checkpointing=self.enable_checkpointing,
            limit_train_batches=0.1 if self.debug else 1.0 
        )

        return trainer

    def _seed_wise_train_validate(
            self, seed: int, curr_log_dir: str, extra_callbacks: list | None = None
        ) -> tuple[str, dict]:
        L.seed_everything(seed)

        train_dl = self.dm.get_train_dl(
            data_path_list=self.train_file,
            batch_size=self.train_bsz,
            sanitise_newsemp_labels=self.sanitise_labels,
            add_noise=self.add_noise_train,
            seed=seed,
            is_newsemp=self.is_newsemp_main,
            do_augment=self.do_augment,
            lbl_split=self.lbl_split
        )

        trainer = self._prepare_trainer(curr_log_dir=curr_log_dir, extra_callbacks=extra_callbacks)

        # https://lightning.ai/docs/pytorch/stable/advanced/model_init.html
        if os.path.exists(curr_log_dir) and not self.do_tune:
            log_info(logger, f"Seed-level logging directory already exists: {curr_log_dir}. So, validating on the saved ckpt...")
            ckpt_list = list(Path(curr_log_dir).rglob("*.ckpt"))
            assert len(ckpt_list) == 1, f"Number of ckpt is not 1."
            best_model_ckpt = ckpt_list[0]
        else:
            log_info(logger, f"Training ...")  
            with trainer.init_module():
                # model created here directly goes to GPU
                model = LitPairedTextModel(
                    plm_names=self.plm_names,
                    lr=self.lr,
                    log_dir=curr_log_dir,
                    save_uc_metrics=self.save_uc_metrics,
                    error_decay_factor=self.error_decay_factor,
                    lambdas=self.lambdas,
                    approach=self.approach,
                    sep_token_id=self.dm.tokeniser.sep_token_id,
                    num_passes=self.num_passes,
                    wd=self.wd
                )
            
            trainer.fit(
                model=model,
                train_dataloaders=train_dl,
                val_dataloaders=self.val_dl
            )

            # getting the best model from the trainer
            best_model_ckpt = trainer.checkpoint_callback.best_model_path
        
        # final validation at the end of training
        log_info(logger, f"Loading the best model from {best_model_ckpt}")
        with trainer.init_module(empty_init=True):
            model = LitPairedTextModel.load_from_checkpoint(best_model_ckpt)

        trainer.validate(model=model, dataloaders=self.val_dl)

        metrics = {key: value.item() for key, value in trainer.callback_metrics.items()}

        if isinstance(trainer.logger, WandbLogger):
            trainer.logger.experiment.finish()

        return best_model_ckpt, metrics
    
    def evaluate(self, model_path: str):
        tester = L.Trainer(
            logger=False,
            devices=1,
            max_epochs=1
        )

        with tester.init_module(empty_init=True):
            model = LitPairedTextModel.load_from_checkpoint(model_path)

        tester.test(model=model, dataloaders=self.test_dl, verbose=True)

        metrics = {key: value.item() for key, value in tester.callback_metrics.items()}
        
        return metrics
    
    def train_test(
        self,
        seeds: list[int],
        parent_log_dir: str
    ) -> None:

        results = []
        for seed in seeds:
            log_info(logger, f"Current seed: {seed}")

            # releasing the memory if allocated
            # torch.cuda.empty_cache()

            curr_log_dir = os.path.join(parent_log_dir, f"seed_{seed}")
            if self.do_train:
                best_model_ckpt, metrics = self._seed_wise_train_validate(
                    seed=seed,
                    curr_log_dir=curr_log_dir
                )
            else:
                best_model_ckpt = list(Path(curr_log_dir).rglob("*.ckpt"))
                assert len(best_model_ckpt) == 1, f"Found {len(best_model_ckpt)} ckpt files."
                best_model_ckpt = best_model_ckpt[0]
                metrics = {} # empty val metrics
            
            if self.do_test:
                # subsequent testing
                log_info(logger, f"Testing using: {best_model_ckpt}")
                test_metrics = self.evaluate(best_model_ckpt)
                metrics = {**metrics, **test_metrics} # merge the two dictionaries 

            metrics["seed"] = seed
            log_info(logger, f"Metrics: {metrics}")
            results.append(metrics)
        save_as = os.path.join(parent_log_dir, f"results_val-{self.do_train}_test-{self.do_test}.csv")
        process_seedwise_metrics(results, save_as)

    def optuna_objective(self, trial: optuna.trial.Trial, optuna_seed: int, optuna_log_dir: str) -> float:
        lambda_1 = trial.suggest_float("lambda_1", 0.0, 50.0, step=0.1)
        lambda_2 = trial.suggest_float("lambda_2", 0.0, 50.0, step=0.1)
        self.lambdas = [1.0, lambda_1, lambda_2]
        
        # self.error_decay_factor = trial.suggest_float("error_decay_factor", 0.0, 3.0, step=0.5)

        pruning_callback = PyTorchLightningPruningCallback(trial, monitor="val_ccc")
        _, metrics = self._seed_wise_train_validate(
            seed=optuna_seed,
            curr_log_dir=optuna_log_dir,
            extra_callbacks=[pruning_callback]
        )

        # releasing the memory - trying to solve that OOM issue with slurm job 
        torch.cuda.empty_cache()

        return metrics["val_ccc"]

    def tune_train_test(self, n_trials: int, parent_log_dir: str, seeds: list[int] = [0]) -> None:
        if self.do_tune:
            optuna_log_dir = os.path.join(parent_log_dir, "optuna_logs")
            os.makedirs(optuna_log_dir, exist_ok=True)
            study = optuna.create_study(
                direction="maximize",
                pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5),
                study_name=self.expt_name,
                storage=f"sqlite:///{optuna_log_dir}/optuna.db",
                load_if_exists=True
            )

            optuna_seed = seeds[0] # using the first seed for optuna tuning
            
            objective_params = partial(
                self.optuna_objective,
                optuna_seed=optuna_seed,
                optuna_log_dir=optuna_log_dir
            )

            study.optimize(objective_params, n_trials=n_trials, show_progress_bar=False)

            study.trials_dataframe().to_csv(os.path.join(optuna_log_dir, "trial_results.csv"), index=False)
            
            best_trial = study.best_trial
            log_info(logger, f"Best trial:{best_trial.value}")

            with open(os.path.join(optuna_log_dir, "best_trial_params.txt"), 'w') as f:
                for key, value in best_trial.params.items():
                    f.write(f"{key}: {value}\n")
            
            fig_slice = optuna.visualization.plot_slice(study)
            fig_slice.write_image(os.path.join(parent_log_dir, "Optuna_slice.pdf"))

            fig_imp = optuna.visualization.plot_param_importances(study)
            fig_imp.write_image(os.path.join(parent_log_dir, "Optuna_param_importances.pdf"))
            
            if self.do_train:
                raise ValueError("Train right after tune with updated parameters is not using updated parameters. Set the parameters and train separately after tuning.")
                                #  " update is no longer correct as there is no self.lambda_1 for example."
            for key, value in best_trial.params.items():
                setattr(self, key, value)
        
        if self.do_train or self.do_test:
            self.train_test(seeds=seeds, parent_log_dir=parent_log_dir)
