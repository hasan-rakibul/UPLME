save_predictions_to_disk: False # True or False

train_data: [2024, 2022]

val_data: 2024 # test would be same as val_data as in in-sample setting

llm: llama # llama or gpt

enable_early_stopping: True
early_stopping_start_epoch: 5

num_epochs: 20

#### Hyperparameters to search
# lrs: [1.0e-5, 2.0e-5, 3.0e-5]
# batch_sizes: [16, 32]

#### Selected hyperparameters
lrs: [3.0e-5]
batch_sizes: [16]

loss_weights: [50, 25]

logging_dir: ./log
freeze_plm: False

# FIXME: these are hardcoded in the latest models
adamw_beta1: 0.9
adamw_beta2: 0.98
adamw_eps: 1.0e-06
adamw_weight_decay: 0.1

lr_scheduler_type: linear # False, linear, polynomial, plateau

warmup_ratio: 0.06
# num_warmup_steps: 150

# plateau_patience: 1
# plateau_factor: 0.05
# plateau_threshold: 1.0e-4


## Error-based noise modelling
# noise_level: 0.4
# num_agents: 3
# save_ensembles_to_disk: True # saves indices and updated train_dl file
# updated_train_dl_file: False
