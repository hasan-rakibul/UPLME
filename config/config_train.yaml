save_predictions_to_disk: False # True or False

noise_level: 0.4
num_agents: 3
save_ensembles_to_disk: True # saves indices and updated train_dl file
updated_train_dl_file: False

train_data: [2024, 2022]
# train_human_portion: 1.0 # 0.8 means additional 80% of train_data[1:] will be used for training. First element of train_data is always 100% used for training.

val_data: 2024 # test would be same as val_data as in in-sample setting

llm: llama # llama or gpt

expt_name_postfix: "loss-weights-10-no-penalty" # postfix to be added in the experiment name
# overwrite_logging_dir: 
# resume_train_from_checkpoint: 

enable_early_stopping: True
early_stopping_start_epoch: 5

num_epochs: 20

#### Hyperparameters to search
# lrs: [1.0e-5, 2.0e-5, 3.0e-5]
# batch_sizes: [16, 32]

#### Selected hyperparameters
lrs: [3.0e-5]
batch_sizes: [16]

logging_dir: ./log
freeze_plm: False

adamw_beta1: 0.9
adamw_beta2: 0.98
adamw_eps: 1.0e-06
adamw_weight_decay: 0.1

lr_scheduler_type: linear # False, linear, polynomial, plateau

warmup_ratio: 0.06
# num_warmup_steps: 150

# plateau_patience: 1
# plateau_factor: 0.05
# plateau_threshold: 1.0e-4
