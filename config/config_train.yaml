debug_mode: False # will run only 1 epoch of training with 1% of training data, saves logs in tmp/ directory

save_predictions_to_disk: False # True or False

run_agentic: False

train_data: [2024, 2022]
# train_human_portion: 1.0 # 0.8 means additional 80% of train_data[1:] will be used for training. First element of train_data is always 100% used for training.

val_data: 2024 # test would be same as val_data as in in-sample setting

llm: llama # llama or gpt

expt_name_postfix: basic # postfix to be added on top of main_label, train_data and train_only_llm_data
# overwrite_logging_dir: logs/20241117_234401_y'(2024,2023,2022)-Data2022 
# resume_train_from_checkpoint: logs/20241023_205914_vanilla+2023LLM/lightning_logs/version_17110004/checkpoints/epoch=5-step=378.ckpt

# finetune_from_checkpoint: logs/20241118_182138_y(2024)-y_llm(2022)/lr_3e-05_bs_16/seed_100/lightning_logs/version_18411165/checkpoints/epoch=10-step=2156.ckpt 
# finetune_lr: 3.0e-6

enable_early_stopping: True
early_stopping_start_epoch: 5

num_epochs: 20
delta: 0.5

#### Hyperparameters to search
# lrs: [1.0e-5, 2.0e-5, 3.0e-5]
# batch_sizes: [16, 32]

#### Selected hyperparameters
lrs: [3.0e-5]
batch_sizes: [16]

logging_dir: ./logs
freeze_plm: False

adamw_beta1: 0.9
adamw_beta2: 0.98
adamw_eps: 1.0e-06
adamw_weight_decay: 0.1

lr_scheduler_type: linear # False, linear, polynomial, plateau

warmup_ratio: 0.06
# num_warmup_steps: 150

# plateau_patience: 1
# plateau_factor: 0.05
# plateau_threshold: 1.0e-4

# #### agentic approach
# updated_train_dl_file: False

noise_level: 0.4
num_agents: 3
save_agentics_to_disk: True # saves indices and updated train_dl file
