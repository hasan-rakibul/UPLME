train_data: [2024, 2022]
val_data: 2024 # test would be same as val_data as in in-sample setting

num_epochs: 20
lr: 3.0e-5
train_bsz: 16
eval_bsz: 16
parent_log_dir: ./log

n_trials: 100

# seeds: [0, 42, 100, 999, 1234]
seeds: [0]
# plm_names: ["roberta-base", "cardiffnlp/twitter-roberta-base-sentiment-latest"]
# plm: siebert/sentiment-roberta-large-english

delta: 2.0

# all files list that we can use for training / validation / testing
article: data/NewsEmp2022/articles_adobe_AMT.csv

2024:
    train: data/NewsEmp2024/trac3_EMP_train.csv
    train_llama: data/NewsEmp2024/trac3_EMP_train_llama.tsv
    train_gpt: data/NewsEmp2024/trac3_EMP_train_gpt.tsv
    val: data/NewsEmp2024/trac3_EMP_dev.csv
    val_llama: data/NewsEmp2024/trac3_EMP_dev_llama.tsv
    val_gpt: data/NewsEmp2024/trac3_EMP_train_gpt.tsv
    test: data/NewsEmp2024/test_data_with_labels/goldstandard_EMP.csv
    test_llama: data/NewsEmp2024/test_data_with_labels/goldstandard_EMP_llama.tsv
2023:
    train: data/NewsEmp2023/WASSA23_essay_level_with_labels_train.tsv
    train_llama: data/NewsEmp2023/WASSA23_essay_level_with_labels_train_llama.tsv
    val: data/NewsEmp2023/WASSA23_essay_level_dev.tsv 
    val_llama: data/NewsEmp2023/WASSA23_essay_level_dev_llama.tsv
    test: data/NewsEmp2023/WASSA23_essay_level_test.tsv
2022:
    train: data/NewsEmp2022/messages_train_ready_for_WS.tsv
    train_llama: data/NewsEmp2022/messages_train_ready_for_WS_llama.tsv
    val: data/NewsEmp2022/messages_dev_features_ready_for_WS_2022.tsv
    val_llama: data/NewsEmp2022/messages_dev_features_ready_for_WS_2022_llama.tsv
    test: data/NewsEmp2022/messages_test_features_ready_for_WS_2022.tsv
